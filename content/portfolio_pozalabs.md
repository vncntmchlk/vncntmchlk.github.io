+++
title = "Portfolio Pozalabs"
+++

Here is a selection of snippets first from my compositions for dance pieces and second from my experimental and research works.
제가 작업한 무용 작품의 일부를 채취한 샘플들 입니다. 다음으로는 제가 실험하고 리서치한 작품들의 샘플 입니다. 


### Electronic music compositions for dance pieces 
### 무용작품과 작업한 샘플들

---
#### TachoTinta / commonnorm (2022)

- Moving Tables {{<audio src="/audio/commonnorm/moving_tables_excerpt.mp3" caption="" >}}
- Running {{<audio src="/audio/commonnorm/running_excerpt.mp3" caption="" >}}
- Zig Zag {{<audio src="/audio/commonnorm/zig_zag_excerpt.mp3" caption="" >}}

#### Flying Elephant (2023/2022) 

- Friendly Hunting {{<audio src="/audio/snippets/friendly hunting.mp3" caption="" >}}
- Loose Connection {{<audio src="/audio/snippets/loose connection.mp3" caption="" >}}

#### TachoTinta / Ultramarine Sway (2023)

- Coral {{<audio src="/audio/ultramarine/coral.mp3" caption="" >}}
- Fishes {{<audio src="/audio/ultramarine/fishes 1.mp3" caption="" >}}

#### Bound by Body / Dancefilm (2021)

{{< youtube Ghv25vW3-gs >}}

#### TachoTinta / Cultural Drag (2020) 

- Yellow twins {{<audio src="/audio/cultural_drag/yellow_twins_excerpt.mp3">}}
&nbsp;
### Experimental and research works // 실험 및 연구 작업

---
#### Hackmeck / treefingers' breeze (2022) 

> I want to show specific examples of machine learning algorithms used in this performance. 
>  First is imitation and variation of the flute by a neural net controlled synthesizer. 

> 이작품에서는 머신 러닝 알고리즘의 구체적인 예를 보여주는 보여 주고 싶었고, 그것을 이용한 퍼포먼스를 하였다. 
먼져 neural net 신디사이저 제어를 이용하여, 플루트 연주의 모방과 변형을 이용하여 연주 했습니다. 


{{< youtube XZvti3zyIfA >}}
- recordings from another performance 다른공연의 레코딩 
{{<audio src="/audio/snippets/treefingers snippet 14.mp3" caption="" >}}
{{<audio src="/audio/snippets/treefingers snippet 11.mp3" caption="" >}}
> The next two examples are the flute live input cut to samples at the onset and sorted by the "brightness" of the sound automatically.
> The samples are then played back with rhythms generated by cellular automata. Some of these processes can be seen in the visuals in the background.

> 다른 두 예로는 플루트의 라이브 연주가 입력이 될때 여러 샘플로 편집되어 소리의 밝기에 따라 자동으로 정렬된 것입니다. 
그런 다름 이 샘플들이 cellular automata를 통해 생성한 리듬으로 재생됩니다. 이러한 과정이 시작적으로 변형되어 배경으로 나타납니다. 

{{< youtube hJWaRuIp2SA >}}
{{< youtube CoT8CA1p7Ro >}}

#### Hackmeck / growing in the dark (2021)
> In this excerpt polyphonic structures are created and manipulated algorithmically. The source is an algorithm called L-System, the visualization is derived
> from the same algorithm.

> 이 부분은 여러 멜로디 구조가 알고리즘을 통해 생성되고 조작됩니다. L-System 이라고 불리는 알고리즘 소스로, 시각화 또한 동일한 알고리즘에서 파생되었습니다. 


{{< youtube OgcEjjUsvV0 >}}

#### Hackmeck / self-assemblers (2021)
> Here i want to show my self-made instrument which allows me to generate rhythmic patterns from geometry. I used a raspberry pi with a camera and made a python program to detect the objects on the table and convert them into rhythms and a visualization.

> 여기서 제가 보여주고자 했던것은 제가 직접 만든 악기를 통해 기하학적 리듬 패턴을 생성하는 작업입니다. 저는 카메라가 달린 raspberry pi 를 이용하여 테이블 위의 물체를 감지하고, 리듬과 시각화로 변화하는 python 프로그램을 만들었습니다.



{{< youtube gJXZT4nBEZI >}}

#### Rhythm generating audioplugin // 리듬 생성 오디오 플러그인
> This is a short demo of an audioplugin i have been working on which generates simple midi patterns from cellular automata, which can be modified.
> The output is beat and tempo synced with the DAW, it is intended to be used alongside other instruments.
> It is still alpha, but the basic function can be seen in the following demo video.

> 이것은 제가 작업 중인 오디로 플러그인의 짧은 데모로, cellular automata 에서 간단한 미디 패턴을 생성하고, 수정할 수 있습니다.
출력은 DAW와 동기화된 비트와 템포이며, 다른 악기와 함께 사용하기 위한 것입니다.
아직 알파 버전이지만 기본 기능은 다음 데모 영상에서 확인하실 수 있습니다.


{{< youtube AjgvXUbNc_8 >}}

---
Thank you for looking at my portfolio!
지금까지 제 포트롤리오를 봐주셔서 감사합니다! 
